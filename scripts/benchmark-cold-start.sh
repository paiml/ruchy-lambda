#!/bin/bash
# Local Cold Start Benchmark (bashrs-compliant)
# Measures ACTUAL cold start performance of Ruchy Lambda runtime

set -euo pipefail

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
readonly BOOTSTRAP="$PROJECT_ROOT/target/release/bootstrap"
readonly RESULTS_FILE="$PROJECT_ROOT/local-benchmark-results.json"
readonly ITERATIONS=10

# lambda-perf baseline cold start times (ms)
readonly CPP_BASELINE="13.54"
readonly RUST_BASELINE="16.98"
readonly GO_BASELINE="45.77"
readonly TARGET="8.0"

# Simple benchmark without complex server setup
# Measures process startup time only (real cold start metric)

printf "üöÄ Ruchy Lambda - Cold Start Benchmark\\n"
printf "========================================\\n\\n"

# Check binary exists
if [ ! -f "$BOOTSTRAP" ]; then
    printf "‚ùå Bootstrap binary not found: %s\\n" "$BOOTSTRAP" >&2
    printf "Run: cargo build --release -p ruchy-lambda-bootstrap\\n" >&2
    exit 1
fi

printf "‚úÖ Binary: %s (%s)\\n\\n" "$BOOTSTRAP" "$(ls -lh "$BOOTSTRAP" | awk '{print $5}')"

# Measure cold start: process creation + initialization only
# This is what lambda-perf actually measures (not full request)

printf "üìä Measuring cold start (process initialization)...\\n\\n"

declare -a times

for i in $(seq 1 "$ITERATIONS"); do
    # Measure: fork + exec + initialization
    # Use /usr/bin/time for accurate measurement
    start_ms=$(date +%s%3N)

    # Start process, let it fail immediately (no Runtime API available)
    # The time to fail IS the cold start time
    timeout 0.1s "$BOOTSTRAP" 2>/dev/null || true

    end_ms=$(date +%s%3N)
    duration=$((end_ms - start_ms))

    times+=("$duration")
    printf "  Iteration %d: %dms\\n" "$i" "$duration"
done

printf "\\n"

# Calculate statistics
sum=0
min=${times[0]}
max=${times[0]}

for val in "${times[@]}"; do
    sum=$((sum + val))
    if [ "$val" -lt "$min" ]; then
        min=$val
    fi
    if [ "$val" -gt "$max" ]; then
        max=$val
    fi
done

avg=$((sum / ${#times[@]}))

printf "üìà Results:\\n"
printf "========================================\\n\\n"
printf "Cold Start Performance:\\n"
printf "  Average: %dms\\n" "$avg"
printf "  Min:     %dms\\n" "$min"
printf "  Max:     %dms\\n" "$max"
printf "\\n"

# Compare vs baselines (simple integer comparison)
printf "üèÜ Comparison vs lambda-perf baselines:\\n"
printf "========================================\\n"
printf "  C++:     %sms\\n" "$CPP_BASELINE"
printf "  Rust:    %sms\\n" "$RUST_BASELINE"
printf "  Go:      %sms\\n" "$GO_BASELINE"
printf "  Ruchy:   %dms\\n" "$avg"
printf "\\n"

# Check target
if [ "$avg" -lt "${TARGET%.*}" ]; then
    printf "‚úÖ Target (<%sms): ACHIEVED!\\n" "$TARGET"
else
    printf "‚ùå Target (<%sms): NOT MET\\n" "$TARGET"
fi

printf "\\n"

# Save results
cat > "$RESULTS_FILE" << EOF
{
  "binary_size_kb": $(stat -c%s "$BOOTSTRAP" | awk '{print int($1/1024)}'),
  "cold_start_ms": {
    "avg": $avg,
    "min": $min,
    "max": $max,
    "iterations": $ITERATIONS
  },
  "comparison": {
    "cpp_baseline": "$CPP_BASELINE",
    "rust_baseline": "$RUST_BASELINE",
    "go_baseline": "$GO_BASELINE"
  },
  "target_ms": "$TARGET",
  "target_met": $([ "$avg" -lt "${TARGET%.*}" ] && printf "true" || printf "false")
}
EOF

printf "üíæ Results saved to: %s\\n" "$RESULTS_FILE"
